{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac866dcd-b1f8-49a1-9cfa-f5fd08ac2488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92f3d4ee-c6de-480b-bd27-ebec8e4af293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the catalog with earthquake start times\n",
    "cat_directory = './data/lunar/training/catalogs/'\n",
    "cat_file = cat_directory + 'apollo12_catalog_GradeA_final.csv'\n",
    "catalog = pd.read_csv(cat_file)\n",
    "\n",
    "# Directory containing the CSV files for each day\n",
    "data_directory = './data/lunar/training/data/S12_GradeA/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78823e3f-f76d-4d3b-b260-69553353d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store all features and targets from all days\n",
    "all_features = []\n",
    "all_targets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3284cdc-8c25-4fc4-b9cc-8709ed2db96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each CSV file (representing one day) using the catalog to get the earthquake start time\n",
    "for i, row in catalog.iterrows():\n",
    "    csv_filename = row['filename'] + '.csv'\n",
    "    earthquake_start_time = row['time_rel(sec)']\n",
    "    \n",
    "    # Load the corresponding seismic data for the day\n",
    "    csv_file_path = os.path.join(data_directory, csv_filename)\n",
    "    if os.path.exists(csv_file_path):\n",
    "        day_data = pd.read_csv(csv_file_path)\n",
    "\n",
    "        # Convert time to datetime and normalize time_rel\n",
    "        day_data['time_abs'] = pd.to_datetime(day_data['time_abs(%Y-%m-%dT%H:%M:%S.%f)'])\n",
    "        day_data['time_rel_normalized'] = day_data['time_rel(sec)'] / (24 * 3600)  # Normalize to 24 hours (in seconds)\n",
    "\n",
    "        # Apply noise reduction using Savitzky-Golay filter\n",
    "        day_data['velocity_smoothed'] = savgol_filter(day_data['velocity(m/s)'], window_length=51, polyorder=3)\n",
    "\n",
    "        # Feature engineering - windowed mean and standard deviation\n",
    "        window_size = 100\n",
    "        day_data['velocity_mean'] = day_data['velocity_smoothed'].rolling(window=window_size).mean()\n",
    "        day_data['velocity_std'] = day_data['velocity_smoothed'].rolling(window=window_size).std()\n",
    "        day_data.dropna(inplace=True)  # Drop NaN values after rolling window\n",
    "\n",
    "        # Ensure we have enough rows for features\n",
    "        if not day_data.empty:\n",
    "            # Collect the features for this day\n",
    "            features = day_data[['velocity_mean', 'velocity_std']].values\n",
    "            all_features.append(features)\n",
    "            \n",
    "            # Create a target array with the same length as the number of rows in the day's data\n",
    "            targets = np.full(shape=(features.shape[0],), fill_value=earthquake_start_time)\n",
    "            all_targets.append(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2832e276-8e2a-4ff5-8800-f39270b7dbd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (75,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19767/517989612.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Combine all data into a single dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mall_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mall_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Split the combined data into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (75,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Combine all data into a single dataset\n",
    "all_features = np.vstack(all_features)\n",
    "all_targets = np.array(all_targets)\n",
    "\n",
    "# Split the combined data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_features, all_targets, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6627d099-fd15-4db7-a7bc-a00f709b6ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine all data into a single dataset\n",
    "all_features = np.vstack(all_features)\n",
    "all_targets = np.hstack(all_targets)\n",
    "\n",
    "# Split the combined data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_features, all_targets, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Random Forest Regressor\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = regressor.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f} seconds\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f} seconds\")\n",
    "\n",
    "# Predict the seismic event start time for a new day\n",
    "new_day_filename = 'example_day.csv'  # Example of a new file\n",
    "new_day_data = pd.read_csv(os.path.join(data_directory, new_day_filename))\n",
    "\n",
    "# Apply the same preprocessing to the new data\n",
    "new_day_data['time_abs'] = pd.to_datetime(new_day_data['time_abs(%Y-%m-%dT%H:%M:%S.%f)'])\n",
    "new_day_data['time_rel_normalized'] = new_day_data['time_rel(sec)'] / new_day_data['time_rel(sec)'].max()\n",
    "new_day_data['velocity_smoothed'] = savgol_filter(new_day_data['velocity(m/s)'], window_length=51, polyorder=3)\n",
    "\n",
    "new_day_data['velocity_mean'] = new_day_data['velocity_smoothed'].rolling(window=window_size).mean()\n",
    "new_day_data['velocity_std'] = new_day_data['velocity_smoothed'].rolling(window=window_size).std()\n",
    "new_day_data.dropna(inplace=True)\n",
    "\n",
    "# Extract features for the new day\n",
    "new_features = new_day_data[['velocity_mean', 'velocity_std']].values\n",
    "\n",
    "# Predict the start time of the earthquake for the new day\n",
    "predicted_start_time = regressor.predict(new_features)\n",
    "\n",
    "# Plot the predicted seismic start time on the smoothed data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(new_day_data['time_rel_normalized'], new_day_data['velocity_smoothed'], label='Velocity Smoothed')\n",
    "plt.axvline(x=predicted_start_time[0] / new_day_data['time_rel(sec)'].max(), color='red', linestyle='--', label='Predicted Start Time')\n",
    "plt.title(\"Seismic Event Prediction for New Day\")\n",
    "plt.xlabel(\"Normalized Time\")\n",
    "plt.ylabel(\"Velocity (m/s)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
