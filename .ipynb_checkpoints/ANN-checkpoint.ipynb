{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac866dcd-b1f8-49a1-9cfa-f5fd08ac2488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92f3d4ee-c6de-480b-bd27-ebec8e4af293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the catalog with earthquake start times\n",
    "cat_directory = './data/lunar/training/catalogs/'\n",
    "cat_file = cat_directory + 'apollo12_catalog_GradeA_final.csv'\n",
    "catalog = pd.read_csv(cat_file)\n",
    "\n",
    "# Directory containing the CSV files for each day\n",
    "data_directory = './data/lunar/training/data/S12_GradeA/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78823e3f-f76d-4d3b-b260-69553353d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store all features and targets from all days\n",
    "all_features = []\n",
    "all_targets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3284cdc-8c25-4fc4-b9cc-8709ed2db96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each CSV file (representing one day) using the catalog to get the earthquake start time\n",
    "for i, row in catalog.iterrows():\n",
    "    csv_filename = row['filename'] + '.csv'\n",
    "    earthquake_start_time = row['time_rel(sec)']\n",
    "    \n",
    "    # Load the corresponding seismic data for the day\n",
    "    csv_file_path = os.path.join(data_directory, csv_filename)\n",
    "    if os.path.exists(csv_file_path):\n",
    "        day_data = pd.read_csv(csv_file_path)\n",
    "\n",
    "        # Convert time to datetime and normalize time_rel\n",
    "        day_data['time_abs'] = pd.to_datetime(day_data['time_abs(%Y-%m-%dT%H:%M:%S.%f)'])\n",
    "        day_data['time_rel_normalized'] = day_data['time_rel(sec)'] / (24 * 3600)  # Normalize to 24 hours (in seconds)\n",
    "\n",
    "        # Apply noise reduction using Savitzky-Golay filter\n",
    "        day_data['velocity_smoothed'] = savgol_filter(day_data['velocity(m/s)'], window_length=51, polyorder=3)\n",
    "\n",
    "        # Feature engineering - windowed mean and standard deviation\n",
    "        window_size = 100\n",
    "        day_data['velocity_mean'] = day_data['velocity_smoothed'].rolling(window=window_size).mean()\n",
    "        day_data['velocity_std'] = day_data['velocity_smoothed'].rolling(window=window_size).std()\n",
    "        day_data.dropna(inplace=True)  # Drop NaN values after rolling window\n",
    "\n",
    "        # Collect the features and target for this day\n",
    "        features = day_data[['velocity_mean', 'velocity_std']].values\n",
    "        all_features.append(features)\n",
    "        all_targets.append(earthquake_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2832e276-8e2a-4ff5-8800-f39270b7dbd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [42693089, 75]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19767/517989612.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Split the combined data into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2655\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one array required as input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    458\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [42693089, 75]"
     ]
    }
   ],
   "source": [
    "# Combine all data into a single dataset\n",
    "all_features = np.vstack(all_features)\n",
    "all_targets = np.array(all_targets)\n",
    "\n",
    "# Split the combined data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_features, all_targets, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6627d099-fd15-4db7-a7bc-a00f709b6ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine all data into a single dataset\n",
    "all_features = np.vstack(all_features)\n",
    "all_targets = np.hstack(all_targets)\n",
    "\n",
    "# Split the combined data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_features, all_targets, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Random Forest Regressor\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = regressor.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f} seconds\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f} seconds\")\n",
    "\n",
    "# Predict the seismic event start time for a new day\n",
    "new_day_filename = 'example_day.csv'  # Example of a new file\n",
    "new_day_data = pd.read_csv(os.path.join(data_directory, new_day_filename))\n",
    "\n",
    "# Apply the same preprocessing to the new data\n",
    "new_day_data['time_abs'] = pd.to_datetime(new_day_data['time_abs(%Y-%m-%dT%H:%M:%S.%f)'])\n",
    "new_day_data['time_rel_normalized'] = new_day_data['time_rel(sec)'] / new_day_data['time_rel(sec)'].max()\n",
    "new_day_data['velocity_smoothed'] = savgol_filter(new_day_data['velocity(m/s)'], window_length=51, polyorder=3)\n",
    "\n",
    "new_day_data['velocity_mean'] = new_day_data['velocity_smoothed'].rolling(window=window_size).mean()\n",
    "new_day_data['velocity_std'] = new_day_data['velocity_smoothed'].rolling(window=window_size).std()\n",
    "new_day_data.dropna(inplace=True)\n",
    "\n",
    "# Extract features for the new day\n",
    "new_features = new_day_data[['velocity_mean', 'velocity_std']].values\n",
    "\n",
    "# Predict the start time of the earthquake for the new day\n",
    "predicted_start_time = regressor.predict(new_features)\n",
    "\n",
    "# Plot the predicted seismic start time on the smoothed data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(new_day_data['time_rel_normalized'], new_day_data['velocity_smoothed'], label='Velocity Smoothed')\n",
    "plt.axvline(x=predicted_start_time[0] / new_day_data['time_rel(sec)'].max(), color='red', linestyle='--', label='Predicted Start Time')\n",
    "plt.title(\"Seismic Event Prediction for New Day\")\n",
    "plt.xlabel(\"Normalized Time\")\n",
    "plt.ylabel(\"Velocity (m/s)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
